{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af779ee",
   "metadata": {},
   "source": [
    "# Crime Classification\n",
    "This notebook involves classification crime data into distinct categories, illuminating the nature and characteristics of various offences. This classification serves as a cornerstone for deeper analysis, setting the stage for predictive modelling and trend analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130744e2",
   "metadata": {},
   "source": [
    "###  Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121aea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all Neccessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.ticker as ticker\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da914d1",
   "metadata": {},
   "source": [
    "### Load in the cleaned crime dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16567a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the cleaned dataset into a pandas dataframe, print statistics\n",
    "df = pd.read_csv('allcleanedcrimedata', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm that the right data set is loaded \n",
    "df.shape\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec636b5",
   "metadata": {},
   "source": [
    "### Drop columns not needed for the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d217767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns \n",
    "df.drop(['date_occ', 'crm_cd_desc', 'weapon_desc', 'time_occ', 'crm_cd', 'AREA', 'rpt_dist_no', 'Mocodes', 'weapon_used_cd', 'Status', 'LOCATION', 'cross_street', 'LAT', 'LON', 'Year', 'Month', 'Day', 'crime_category'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63dd0b1",
   "metadata": {},
   "source": [
    "### Take a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52801ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random sample of size 5000 from the data\n",
    "crime_data = df.sample(n=5000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97da4e",
   "metadata": {},
   "source": [
    "### One-Hot Encoding with Pandas: Transforming Categorical Data\n",
    "\n",
    "In this section, we are preparing our dataset for machine learning algorithms. Many machine learning models require numerical input, so categorical variables (like crime types, location categories, etc.) need to be transformed into a format that these models can understand.\n",
    "\n",
    "To achieve this, we use Pandas' `get_dummies` function, which implements a technique known as one-hot encoding. This technique converts categorical variable(s) into a form that could be provided to ML algorithms to do a better job in prediction. \n",
    "\n",
    "Here's what `get_dummies` does:\n",
    "- For each unique value in a categorical column, it creates a new column (a dummy variable) and assigns a binary value of 1 or 0.\n",
    "- Each observation in the dataset is then represented with a 1 in the column for its corresponding value and 0 in all other new columns.\n",
    "\n",
    "The code `crime_data = pd.get_dummies(crime_data)` applies this transformation to all the categorical columns in our `crime_data` DataFrame. As a result, we'll have a DataFrame where all categorical features are represented in a way that our machine learning models can efficiently process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c63834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to the feature columns only\n",
    "features = crime_data.drop('crime_type', axis=1)\n",
    "features_encoded = pd.get_dummies(features)\n",
    "\n",
    "# Now split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_encoded, crime_data['crime_type'], test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e5d000",
   "metadata": {},
   "source": [
    "### Applying Robust Scaling to Features\n",
    "\n",
    "### Why Use Robust Scaling?\n",
    "In data preprocessing, scaling of features is a critical step, especially when working with algorithms sensitive to the scale of input data, like Logistic Regression, SVM, and K-Nearest Neighbors. Robust Scaling is particularly effective when the dataset contains outliers. Unlike standard scaling methods that are influenced by outliers, Robust Scaling uses statistics that are robust to outliers (the median and interquartile range) to scale the data.\n",
    "\n",
    "### Implementation of Robust Scaling\n",
    "The process involves the following steps:\n",
    "-    **Instantiate the Scaler**: Create a `RobustScaler` object. This scaler removes the median and scales the data according to the Interquartile Range (IQR).\n",
    "-    **Fit and Transform the Training Data**: We fit the scaler to the `X_train` data and then transform `X_train`. Fitting the scaler involves computing the median and IQR, which are then used to scale the data.\n",
    "-    **Transform the Test Data**: We transform the `X_test` data using the same scaler. It's important to note that we do not fit the scaler to the test data but only transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9814a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Robust scaling\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d58a8",
   "metadata": {},
   "source": [
    "\n",
    "-   Initialize the veral machine learning classifiers from the scikit-learn library. Each of these classifiers will be used to model our crime data and determine the most effective approach for classification. The classifiers we're initializing are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3bd0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "rf = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "lr = LogisticRegression(max_iter=500) \n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of classifiers and their respective parameter grids for hyperparameter tuning\n",
    "classifiers = [('Random Forest', rf, {'n_estimators': [100, 200, 250]}),\n",
    "               ('SVM', svm, {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}),\n",
    "               ('Logistic Regression', lr, {'C': [0.1, 1, 10]}),\n",
    "               ('KNN', knn, {'n_neighbors': [3, 5, 7]})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to store the best estimator for each classifier after hyperparameter tuning\n",
    "best_estimators = {}\n",
    "\n",
    "# Iterate over each classifier and their respective parameter grid\n",
    "for name, classifier, param_grid in classifiers:\n",
    "    # Initialize GridSearchCV with the given classifier, parameter grid, and 5-fold cross-validation\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "    # Fit the GridSearchCV object on the training data\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    # Print the best hyperparameters for the current classifier\n",
    "    print(f'Best hyperparameters for {name}: {grid_search.best_params_}')\n",
    "    # Store the best estimator for the current classifier in the dictionary\n",
    "    best_estimators[name] = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a843409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predicted labels for test data using best models found for each classifier\n",
    "y_pred_rf = best_estimators['Random Forest'].predict(X_test_scaled)\n",
    "y_pred_svm = best_estimators['SVM'].predict(X_test_scaled)\n",
    "y_pred_lr = best_estimators['Logistic Regression'].predict(X_test_scaled)\n",
    "y_pred_knn = best_estimators['KNN'].predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba3cf80",
   "metadata": {},
   "source": [
    "-   Create confusion matrix plot and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='YlGn') \n",
    "    plt.title(title)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "# Iterate over the best estimator dictionary and print the confusion matrix and classification report for each classifier\n",
    "for name, model in best_estimators.items():\n",
    "    # Generate predicted labels for test data using the best model\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    plot_confusion_matrix(cm, f'Confusion Matrix for {name}')\n",
    "\n",
    "    # Printing classification report\n",
    "    print(f'--- Classification Report for {name} ---')\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
